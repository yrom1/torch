{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd4e841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.10.8 (main, Nov 30 2022, 20:34:09) [Clang 14.0.0 (clang-1400.0.29.202)]',\n",
       " '2.0.0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VERSION\n",
    "from sys import version\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "version, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da97497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229],\n",
       "         [-0.1863,  2.2082, -0.6380,  0.4617,  0.2674]]),\n",
       " tensor([-0.1863,  2.2082, -0.6380,  0.4617,  0.2674]),\n",
       " tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229],\n",
       "         [ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229],\n",
       "         [-0.1863,  2.2082, -0.6380,  0.4617,  0.2674]]),\n",
       " tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229],\n",
       "         [-0.1863,  2.2082, -0.6380,  0.4617,  0.2674],\n",
       "         [-0.1863,  2.2082, -0.6380,  0.4617,  0.2674]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INDEXING WITH LISTS & TENSORS IN ONE DIMENSION\n",
    "t = torch.randn((2,5))\n",
    "t, t[1], t[[0,0,1]], t[torch.tensor([0,1,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70467826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([[0, 2],\n",
       "         [1, 1],\n",
       "         [2, 0]]),\n",
       " tensor([0, 1, 2]),\n",
       " tensor([2, 1, 0]),\n",
       " tensor([2, 5, 8]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INDEXING WITH TENSORS IN MULTIPLE DIMENSIONS\n",
    "x = torch.arange(12).reshape(3, 4)\n",
    "y = torch.tensor([[0, 2], [1, 1], [2, 0]])\n",
    "x, y, y[:, 0], y[:, 1], x[y[:, 0], y[:, 1]]  # do you see it for the last one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ae5052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 2]), torch.Size([32, 3]), torch.Size([32, 3, 2]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more fun example from karpathy https://youtu.be/TCH_1BHY58I?t=1028\n",
    "C = torch.randn((27,2))\n",
    "X = torch.arange(32*3).reshape((32,3)) % 27\n",
    "C.shape, X.shape, C[X].shape\n",
    "# if you understand this, you can explain why the % 27 is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ad5103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.5568, -0.8123,  1.1964],\n",
       "         [ 0.8613, -1.3682, -0.7740]]),\n",
       " tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([[ 0,  1,  2,  3,  0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7,  4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11,  8,  9, 10, 11]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONCATENATE\n",
    "X = torch.randn((2,3))\n",
    "X, torch.cat((x, x), 0), torch.cat((x, x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae28b843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " (tensor([0, 1, 2]), tensor([3, 4, 5])),\n",
       " (tensor([0, 3]), tensor([1, 4]), tensor([2, 5])))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNBIND\n",
    "t = torch.arange(6).reshape((2,3))\n",
    "t, t.unbind(), t.unbind(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3def70aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1],\n",
       "          [ 2,  3],\n",
       "          [ 4,  5]],\n",
       " \n",
       "         [[ 6,  7],\n",
       "          [ 8,  9],\n",
       "          [10, 11]]]),\n",
       " (tensor([[0, 1],\n",
       "          [2, 3],\n",
       "          [4, 5]]),\n",
       "  tensor([[ 6,  7],\n",
       "          [ 8,  9],\n",
       "          [10, 11]])))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(12).reshape((2,3,2))\n",
    "t2, t2.unbind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0c411ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-4.], dtype=torch.float64, requires_grad=True),\n",
       " tensor([-20.], dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " 46.0,\n",
       " -4.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MICROGRAD https://github.com/karpathy/micrograd/blob/master/test/test_engine.py\n",
    "# from micrograd.engine import Value\n",
    "\n",
    "# x = Value(-4.0)\n",
    "# z = 2 * x + 2 + x\n",
    "# q = z.relu() + z * x\n",
    "# h = (z * z).relu()\n",
    "# y = h + q + q * x\n",
    "# y.backward()\n",
    "# xmg, ymg = x, y\n",
    "\n",
    "x = torch.Tensor([-4.0]).double()\n",
    "x.requires_grad = True\n",
    "z = 2 * x + 2 + x\n",
    "q = z.relu() + z * x\n",
    "h = (z * z).relu()\n",
    "y = h + q + q * x\n",
    "y.backward() # manual backprod https://youtu.be/VMj-3S1tku0?t=1929\n",
    "# xpt, ypt = x, y\n",
    "\n",
    "# forward pass went well\n",
    "# assert ymg.data == ypt.data.item()\n",
    "# backward pass went well\n",
    "# assert xmg.grad == xpt.grad.item()\n",
    "x, y, x.grad.item(), x.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6210b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y.grad)\n",
    "# /var/folders/8f/pmq_g1556d942470cc0ghx480000gn/T/ipykernel_73007/486760323.py:1: UserWarning:\n",
    "# The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute\n",
    "# won't be populated during autograd.backward(). If you indeed want the .grad field to be populated\n",
    "# for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor\n",
    "# by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531\n",
    "# for more informations. (Triggered internally at \n",
    "# /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccac96ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, False, False, False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NON-LEAFS\n",
    "# Only leaf Tensors will have their grad populated during a call to backward().\n",
    "x.is_leaf, y.is_leaf, h.is_leaf, q.is_leaf, z.is_leaf\n",
    "# some off-hand micrograd talk about leaf nodes https://youtu.be/VMj-3S1tku0?t=1748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "393c63a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, False, True, True, True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(10, requires_grad=True)\n",
    "b = torch.rand(10, requires_grad=True).double()\n",
    "c = torch.rand(10, requires_grad=True) + 2\n",
    "d = torch.rand(10).double()\n",
    "e = torch.rand(10).double().requires_grad_()\n",
    "f = torch.rand(10, requires_grad=True, dtype=torch.double)\n",
    "a.is_leaf, b.is_leaf, c.is_leaf, d.is_leaf, e.is_leaf, f.is_leaf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86de64d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.rand(10, requires_grad=True) # .double()\n",
    "b.is_leaf # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2000e9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17]),\n",
       " tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12, 13, 14, 15, 16, 17]]),\n",
       " torch.Size([2, 9]),\n",
       " tensor([[[ 0,  1,  2],\n",
       "          [ 3,  4,  5],\n",
       "          [ 6,  7,  8]],\n",
       " \n",
       "         [[ 9, 10, 11],\n",
       "          [12, 13, 14],\n",
       "          [15, 16, 17]]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VIEWS\n",
    "a = torch.arange(18)\n",
    "a, a.view(2, 9), a.view(-1, 9).shape, a.view(2,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fa939f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # ya that's a global lol\n",
    "a.storage() # karpathy did his lectures before torch 2.0!\n",
    "# read more about pytorch internals here! :\n",
    "# http://blog.ezyang.com/2019/05/pytorch-internals/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2d03498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5437794688, 5437794688)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MEMORY DATA POINTER\n",
    "t.data_ptr(), t.view(-1, 3).data_ptr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
